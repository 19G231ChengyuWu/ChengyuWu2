{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10fa283b-f07a-45c3-8b0d-7a5875dca8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score without PCA: 0.9047619047619048\n",
      "Precision without PCA: 0.9047619047619048\n",
      "Recall without PCA: 0.9047619047619048\n",
      "F1 score with PCA (first principal component): 0.9243697478991596\n",
      "Precision with PCA (first principal component): 0.9821428571428571\n",
      "Recall with PCA (first principal component): 0.873015873015873\n",
      "F1 score with PCA (first and second principal components): 0.9243697478991596\n",
      "Precision with PCA (first and second principal components): 0.9821428571428571\n",
      "Recall with PCA (first and second principal components): 0.873015873015873\n",
      "False Positives (FP): 1\n",
      "True Positives (TP): 55\n",
      "False Positive Rate (FPR): 0.009259259259259259\n",
      "True Positive Rate (TPR): 0.873015873015873\n",
      "Using continuous data can be beneficial for the model as it contains more information. However, PCA can help reduce the dimensionality of the data, which may lead to faster training and potentially better generalization. In this case, we need to compare the scores to determine if PCA is actually beneficial.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
    "column_names = ['id', 'diagnosis'] + [f'feature_{i}' for i in range(1, 31)]\n",
    "df = pd.read_csv(url, names=column_names)\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "X = df.drop(['id', 'diagnosis'], axis=1)\n",
    "y = df['diagnosis']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=2, min_samples_split=5, max_depth=2)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_no_pca = f1_score(y_test, y_pred)\n",
    "precision_no_pca = precision_score(y_test, y_pred)\n",
    "recall_no_pca = recall_score(y_test, y_pred)\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca_1 = X_train_pca[:, :1]\n",
    "X_test_pca_1 = X_test_pca[:, :1]\n",
    "clf_pca_1 = DecisionTreeClassifier(min_samples_leaf=2, min_samples_split=5, max_depth=2)\n",
    "clf_pca_1.fit(X_train_pca_1, y_train)\n",
    "y_pred_pca_1 = clf_pca_1.predict(X_test_pca_1)\n",
    "f1_pca_1 = f1_score(y_test, y_pred_pca_1)\n",
    "precision_pca_1 = precision_score(y_test, y_pred_pca_1)\n",
    "recall_pca_1 = recall_score(y_test, y_pred_pca_1)\n",
    "X_train_pca_2 = X_train_pca[:, :2]\n",
    "X_test_pca_2 = X_test_pca[:, :2]\n",
    "clf_pca_2 = DecisionTreeClassifier(min_samples_leaf=2, min_samples_split=5, max_depth=2)\n",
    "clf_pca_2.fit(X_train_pca_2, y_train)\n",
    "y_pred_pca_2 = clf_pca_2.predict(X_test_pca_2)\n",
    "f1_pca_2 = f1_score(y_test, y_pred_pca_2)\n",
    "precision_pca_2 = precision_score(y_test, y_pred_pca_2)\n",
    "recall_pca_2 = recall_score(y_test, y_pred_pca_2)\n",
    "cm = confusion_matrix(y_test, y_pred_pca_2)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "FPR = FP / (FP + TN)\n",
    "TPR = TP / (TP + FN)\n",
    "print(f\"F1 score without PCA: {f1_no_pca}\")\n",
    "print(f\"Precision without PCA: {precision_no_pca}\")\n",
    "print(f\"Recall without PCA: {recall_no_pca}\")\n",
    "print(f\"F1 score with PCA (first principal component): {f1_pca_1}\")\n",
    "print(f\"Precision with PCA (first principal component): {precision_pca_1}\")\n",
    "print(f\"Recall with PCA (first principal component): {recall_pca_1}\")\n",
    "print(f\"F1 score with PCA (first and second principal components): {f1_pca_2}\")\n",
    "print(f\"Precision with PCA (first and second principal components): {precision_pca_2}\")\n",
    "print(f\"Recall with PCA (first and second principal components): {recall_pca_2}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"False Positive Rate (FPR): {FPR}\")\n",
    "print(f\"True Positive Rate (TPR): {TPR}\")\n",
    "print(\"Using continuous data can be beneficial for the model as it contains more information. However, PCA can help reduce the dimensionality of the data, which may lead to faster training and potentially better generalization. In this case, we need to compare the scores to determine if PCA is actually beneficial.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226c4ad-ca3a-4237-894c-dbc53f815a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
